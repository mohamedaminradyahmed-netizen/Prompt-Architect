/**
 * DIRECTIVE-035: RLAIF (Reinforcement Learning from AI Feedback)
 * 
 * ÙŠÙ‚Ù„Ù„ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø´Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… AI Ù„Ù„ØªÙ‚ÙŠÙŠÙ… Ø¹Ø¨Ø±:
 * 1. Bootstrap Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´Ø±ÙŠØ© Ø£ÙˆÙ„ÙŠØ©
 * 2. Self-Play Loop Ù„Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø³ØªÙ…Ø±
 * 3. Human-in-the-Loop Validation Ø§Ù„Ø¯ÙˆØ±ÙŠ
 * 
 * Ù„Ù…Ø§Ø°Ø§: ÙŠØ³Ù…Ø­ Ø¨Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„ÙƒØ¨ÙŠØ± Ù„Ù„ØªØ­Ø³ÙŠÙ† Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø¬ÙˆØ¯Ø© Ø¹Ø§Ù„ÙŠØ© Ø¹Ø¨Ø± Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø¨Ø´Ø±ÙŠØ© Ø§Ù„Ø¯ÙˆØ±ÙŠØ©
 */

import { RewardModel, TrainingExample as RewardTrainingExample } from '../models/rewardModel';
import { PromptVariation, MutationType, mutationTypes, tryCatchStyleMutation, reduceContextMutation, expandMutation, constrainMutation } from '../mutations';
import { classifyPrompt, PromptCategory } from '../types/promptTypes';
import { getFeedbackFromStorage, HumanFeedback } from '../api/feedback';
import { collectTrainingData, TrainingExample } from './dataCollection';

// ============================================================================
// INTERFACES
// ============================================================================

/**
 * Policy interface: ÙŠØ­Ø¯Ø¯ ÙƒÙŠÙ Ù†Ø®ØªØ§Ø± mutations
 */
export interface MutationPolicy {
  /**
   * ÙŠØ®ØªØ§Ø± mutation type Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ prompt
   */
  selectMutation(prompt: string, availableMutations: MutationType[]): MutationType;
  
  /**
   * ÙŠØ­Ø³Ù‘Ù† Policy Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ experiences
   */
  update(rewards: Map<MutationType, number[]>): void;
  
  /**
   * ÙŠØ­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø§Ù„Ù€ mutations (Ù„Ù„ØªØ­Ù„ÙŠÙ„)
   */
  getProbabilities(prompt: string, mutations: MutationType[]): Map<MutationType, number>;
}

/**
 * Ù†ØªÙŠØ¬Ø© ØªØ¯Ø±ÙŠØ¨ RLAIF
 */
export interface ImprovedPolicy extends MutationPolicy {
  improvementStats: {
    startingAverageReward: number;
    endingAverageReward: number;
    iterations: number;
    humanValidations: number;
    humanCorrections: number;
  };
}

/**
 * Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª RLAIF Training
 */
export interface RLAIFConfig {
  iterations: number;              // Ø¹Ø¯Ø¯ Ø¯ÙˆØ±Ø§Øª Self-Play
  batchSize: number;               // Ø¹Ø¯Ø¯ variations Ù„ÙƒÙ„ iteration
  humanValidationInterval: number; // Ù…Ø±Ø§Ø¬Ø¹Ø© Ø¨Ø´Ø±ÙŠØ© ÙƒÙ„ N iterations
  bootstrapFromHumanFeedback: boolean; // Ø§Ø³ØªØ®Ø¯Ø§Ù… human feedback Ù„Ù„Ø¨Ø¯Ø§ÙŠØ©
  minHumanFeedbackSamples: number; // Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù…Ù† human feedback Ù„Ù„Ø¨Ø¯Ø¡
}

// ============================================================================
// DEFAULT POLICY (Simple Probability-Based)
// ============================================================================

/**
 * Policy Ø¨Ø³ÙŠØ· ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ù…ØªØ³Ø§ÙˆÙŠØ©
 */
export class DefaultMutationPolicy implements MutationPolicy {
  private mutationRewards: Map<MutationType, number[]>;
  
  constructor() {
    this.mutationRewards = new Map();
    mutationTypes.forEach(m => {
      this.mutationRewards.set(m, []);
    });
  }
  
  selectMutation(prompt: string, availableMutations: MutationType[]): MutationType {
    const probs = this.getProbabilities(prompt, availableMutations);
    
    // Sample from distribution
    const rand = Math.random();
    let cumulative = 0;
    for (const [mutation, prob] of probs.entries()) {
      cumulative += prob;
      if (rand < cumulative) {
        return mutation;
      }
    }
    
    return availableMutations[availableMutations.length - 1];
  }
  
  getProbabilities(prompt: string, mutations: MutationType[]): Map<MutationType, number> {
    const probs = new Map<MutationType, number>();
    const uniformProb = 1.0 / mutations.length;
    
    // Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù„Ø¯ÙŠÙ†Ø§ Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ù…ØªØ³Ø§ÙˆÙŠØ©
    let hasData = false;
    mutations.forEach(m => {
      const rewards = this.mutationRewards.get(m) || [];
      if (rewards.length > 0) hasData = true;
    });
    
    if (!hasData) {
      mutations.forEach(m => probs.set(m, uniformProb));
      return probs;
    }
    
    // Ø§Ø­Ø³Ø¨ Ù…ØªÙˆØ³Ø· reward Ù„ÙƒÙ„ mutation
    const avgRewards = new Map<MutationType, number>();
    mutations.forEach(m => {
      const rewards = this.mutationRewards.get(m) || [];
      if (rewards.length === 0) {
        avgRewards.set(m, 0.5); // Default neutral
      } else {
        const avg = rewards.reduce((a, b) => a + b, 0) / rewards.length;
        avgRewards.set(m, avg);
      }
    });
    
    // Convert to probabilities using softmax
    const exps = new Map<MutationType, number>();
    let sumExp = 0;
    mutations.forEach(m => {
      const exp = Math.exp(avgRewards.get(m)! * 5); // Temperature scaling
      exps.set(m, exp);
      sumExp += exp;
    });
    
    mutations.forEach(m => {
      probs.set(m, (exps.get(m)! / sumExp) || uniformProb);
    });
    
    return probs;
  }
  
  update(rewards: Map<MutationType, number[]>): void {
    // Merge new rewards
    rewards.forEach((newRewards, mutation) => {
      const existing = this.mutationRewards.get(mutation) || [];
      this.mutationRewards.set(mutation, [...existing, ...newRewards]);
      
      // Keep only last 100 rewards per mutation (sliding window)
      if (this.mutationRewards.get(mutation)!.length > 100) {
        const all = this.mutationRewards.get(mutation)!;
        this.mutationRewards.set(mutation, all.slice(-100));
      }
    });
  }
}

// ============================================================================
// BOOTSTRAP FROM HUMAN FEEDBACK
// ============================================================================

/**
 * Bootstrap Reward Model Ù…Ù† human feedback
 */
async function bootstrapRewardModel(
  rewardModel: RewardModel,
  minSamples: number
): Promise<{ success: boolean; samplesUsed: number }> {
  // Ø¬Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…ØªØ¹Ø¯Ø¯Ø©
  const trainingExamples: RewardTrainingExample[] = [];
  
  // 1. Ø¬Ù…Ø¹ Ù…Ù† human feedback Ù…Ø¨Ø§Ø´Ø±Ø©
  const feedbacks = getFeedbackFromStorage();
  for (const feedback of feedbacks) {
    // Ù†Ø­ØªØ§Ø¬ original prompt - Ù‚Ø¯ Ù†Ø­ØªØ§Ø¬ Ù„ØªØ®Ø²ÙŠÙ†Ù‡ Ù…Ø¹ variation
    // Ù„Ù„Ø¢Ù† Ø³Ù†Ø³ØªØ®Ø¯Ù… feedback ÙƒØªÙ‚Ø±ÙŠØ¨
    if (feedback.score >= 1 && feedback.score <= 5) {
      trainingExamples.push({
        id: feedback.id || `feedback_${Date.now()}`,
        originalPrompt: feedback.promptId, // TODO: ÙŠØ¬Ø¨ ØªØ®Ø²ÙŠÙ† original prompt
        modifiedPrompt: feedback.variationId,
        outputs: { original: '', modified: '' },
        humanScore: feedback.score,
        metadata: {
          category: classifyPrompt(feedback.variationId).category,
          mutationType: 'unknown',
          timestamp: feedback.timestamp || new Date(),
          userId: feedback.userId,
        },
      });
    }
  }
  
  // 2. Ø¬Ù…Ø¹ Ù…Ù† training data collection
  for await (const example of collectTrainingData()) {
    if (example.humanScore >= 1 && example.humanScore <= 5) {
      trainingExamples.push({
        id: example.id,
        originalPrompt: example.originalPrompt,
        modifiedPrompt: example.modifiedPrompt,
        context: example.context,
        outputs: example.outputs,
        humanScore: example.humanScore,
        feedback: example.feedback,
        metadata: example.metadata,
      });
    }
  }
  
  if (trainingExamples.length < minSamples) {
    return { success: false, samplesUsed: trainingExamples.length };
  }
  
  // ØªØ¯Ø±ÙŠØ¨ Reward Model
  try {
    rewardModel.train(trainingExamples);
    return { success: true, samplesUsed: trainingExamples.length };
  } catch (error) {
    console.error('Failed to train reward model during bootstrap:', error);
    return { success: false, samplesUsed: trainingExamples.length };
  }
}

// ============================================================================
// APPLY MUTATION
// ============================================================================

/**
 * ÙŠØ·Ø¨Ù‚ mutation Ø¹Ù„Ù‰ prompt ÙˆÙŠØ¹ÙŠØ¯ PromptVariation
 */
function applyMutation(prompt: string, mutationType: MutationType): PromptVariation {
  const category = classifyPrompt(prompt).category;
  
  switch (mutationType) {
    case 'try-catch-style':
      return tryCatchStyleMutation(prompt);
    case 'context-reduction':
      return reduceContextMutation(prompt);
    case 'expansion':
      return expandMutation(prompt);
    case 'constraint-addition':
      return constrainMutation(prompt, category);
    default:
      // Fallback: return original
      return {
        text: prompt,
        mutationType: 'try-catch-style',
        changeDescription: 'No mutation applied',
        expectedImpact: {},
      };
  }
}

// ============================================================================
// HUMAN-IN-THE-LOOP VALIDATION
// ============================================================================

/**
 * Human Validation: ÙŠØ¹Ø±Ø¶ variations Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø¨Ø´Ø±ÙŠØ©
 */
export interface HumanValidationResult {
  validated: boolean;
  corrections: Array<{ variation: PromptVariation; correctReward: number }>;
  avgHumanReward: number;
}

/**
 * ÙŠØ­Ø§ÙƒÙŠ human validation (ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ØŒ Ø³ÙŠÙƒÙˆÙ† UI Ø­Ù‚ÙŠÙ‚ÙŠ)
 */
async function humanValidate(
  variations: Array<{ variation: PromptVariation; aiReward: number }>,
  rewardModel: RewardModel
): Promise<HumanValidationResult> {
  // ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØŒ Ø³ÙŠØªÙ… Ø¹Ø±Ø¶ Ù‡Ø°Ù‡ ÙÙŠ UI
  // Ù„Ù„Ø¢Ù†ØŒ Ù†Ø³ØªØ®Ø¯Ù… feedback Ù…ÙˆØ¬ÙˆØ¯ Ù„Ù„ØªØ­Ù‚Ù‚
  
  const corrections: Array<{ variation: PromptVariation; correctReward: number }> = [];
  let totalReward = 0;
  let validatedCount = 0;
  
  for (const item of variations) {
    // Ø§Ø¨Ø­Ø« Ø¹Ù† human feedback Ù„Ù‡Ø°Ø§ variation
    const feedbacks = getFeedbackFromStorage();
    const relevantFeedback = feedbacks.find(
      f => f.variationId === item.variation.text.substring(0, 50)
    );
    
    if (relevantFeedback) {
      const humanReward = relevantFeedback.score / 5.0; // Normalize to 0-1
      const aiReward = item.aiReward;
      
      // Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ÙØ±Ù‚ ÙƒØ¨ÙŠØ±ØŒ Ø£Ø¶Ù correction
      if (Math.abs(humanReward - aiReward) > 0.2) {
        corrections.push({
          variation: item.variation,
          correctReward: humanReward,
        });
        
        // Ø­Ø¯Ù‘Ø« reward model Ø¨Ù‡Ø°Ù‡ Ø§Ù„ØªØµØ­ÙŠØ­Ø§Øª
        const category = classifyPrompt(item.variation.text).category;
        // Note: Ù†Ø­ØªØ§Ø¬ original prompt Ù‡Ù†Ø§ - Ù„Ù„Ø¨Ø³Ø§Ø·Ø© Ø³Ù†Ø³ØªØ®Ø¯Ù… variation
        rewardModel.train([{
          id: `correction_${Date.now()}`,
          originalPrompt: item.variation.text, // Simplified
          modifiedPrompt: item.variation.text,
          outputs: { original: '', modified: '' },
          humanScore: relevantFeedback.score,
          metadata: {
            category,
            mutationType: item.variation.mutationType,
            timestamp: new Date(),
            userId: relevantFeedback.userId,
          },
        }]);
      }
      
      totalReward += humanReward;
      validatedCount++;
    } else {
      // Ø¨Ø¯ÙˆÙ† feedback Ø¨Ø´Ø±ÙŠØŒ Ø§Ø³ØªØ®Ø¯Ù… AI reward
      totalReward += item.aiReward;
      validatedCount++;
    }
  }
  
  return {
    validated: validatedCount > 0,
    corrections,
    avgHumanReward: validatedCount > 0 ? totalReward / validatedCount : 0,
  };
}

// ============================================================================
// MAIN RLAIF TRAINING FUNCTION
// ============================================================================

/**
 * RLAIF Training Loop
 * 
 * Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©:
 * 1. Bootstrap Ù…Ù† human feedback Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹
 * 2. Self-Play Loop:
 *    - ÙˆÙ„Ù‘Ø¯ variations Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Policy
 *    - Ù‚ÙŠÙ‘Ù…Ù‡Ø§ Ø¨Ù€ Reward Model
 *    - Ø­Ø³Ù‘Ù† Policy Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
 * 3. Human Validation Ø§Ù„Ø¯ÙˆØ±ÙŠ:
 *    - Ø±Ø§Ø¬Ø¹ Ø¹ÙŠÙ†Ø§Øª Ø¯ÙˆØ±ÙŠØ§Ù‹ Ù…Ø¹ Ø¨Ø´Ø±
 *    - ØµØ­Ù‘Ø­ Ø£Ø®Ø·Ø§Ø¡ Reward Model
 *    - Ø£Ø¹Ø¯ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
 */
export async function rlaifTraining(
  initialPolicy: MutationPolicy,
  rewardModel: RewardModel,
  config: Partial<RLAIFConfig> = {}
): Promise<ImprovedPolicy> {
  const fullConfig: RLAIFConfig = {
    iterations: 10,
    batchSize: 20,
    humanValidationInterval: 3,
    bootstrapFromHumanFeedback: true,
    minHumanFeedbackSamples: 10,
    ...config,
  };
  
  console.log('ğŸš€ Starting RLAIF Training...');
  console.log(`Config: ${JSON.stringify(fullConfig, null, 2)}`);
  
  // Wrap policy in ImprovedPolicy
  const improvedPolicy: ImprovedPolicy = {
    ...initialPolicy,
    improvementStats: {
      startingAverageReward: 0,
      endingAverageReward: 0,
      iterations: fullConfig.iterations,
      humanValidations: 0,
      humanCorrections: 0,
    },
  };
  
  // 1. BOOTSTRAP PHASE
  if (fullConfig.bootstrapFromHumanFeedback) {
    console.log('ğŸ“š Bootstrap: Training Reward Model from Human Feedback...');
    const bootstrapResult = await bootstrapRewardModel(rewardModel, fullConfig.minHumanFeedbackSamples);
    
    if (bootstrapResult.success) {
      console.log(`âœ… Bootstrap successful: Used ${bootstrapResult.samplesUsed} human feedback samples`);
    } else {
      console.warn(`âš ï¸ Bootstrap incomplete: Only ${bootstrapResult.samplesUsed} samples (minimum: ${fullConfig.minHumanFeedbackSamples})`);
    }
  }
  
  // 2. SELF-PLAY LOOP
  let currentPrompt = "Write a function to process user input";
  const allRewards: number[] = [];
  
  for (let iteration = 0; iteration < fullConfig.iterations; iteration++) {
    console.log(`\nğŸ”„ Iteration ${iteration + 1}/${fullConfig.iterations}`);
    
    const batchRewards = new Map<MutationType, number[]>();
    const batchVariations: Array<{ variation: PromptVariation; aiReward: number; mutationType: MutationType }> = [];
    
    // Generate batch of variations
    for (let b = 0; b < fullConfig.batchSize; b++) {
      // Select mutation using policy
      const selectedMutation = improvedPolicy.selectMutation(currentPrompt, mutationTypes);
      
      // Apply mutation
      const variation = applyMutation(currentPrompt, selectedMutation);
      
      // Evaluate with Reward Model
      const category = classifyPrompt(variation.text).category;
      const prediction = rewardModel.predict(currentPrompt, variation.text, variation.mutationType, category);
      const reward = prediction.score; // Normalize to 0-1
      
      // Store results
      const existingRewards = batchRewards.get(selectedMutation) || [];
      batchRewards.set(selectedMutation, [...existingRewards, reward]);
      batchVariations.push({ variation, aiReward: reward, mutationType: selectedMutation });
      allRewards.push(reward);
      
      // Update current prompt occasionally (self-play evolution)
      if (reward > 0.7) {
        currentPrompt = variation.text;
      }
    }
    
    // Calculate batch statistics
    const avgReward = allRewards.slice(-fullConfig.batchSize).reduce((a, b) => a + b, 0) / fullConfig.batchSize;
    if (iteration === 0) {
      improvedPolicy.improvementStats.startingAverageReward = avgReward;
    }
    
    console.log(`  Average Reward: ${avgReward.toFixed(4)}`);
    console.log(`  Mutations used: ${Array.from(batchRewards.keys()).join(', ')}`);
    
    // 3. HUMAN VALIDATION (Periodic)
    if ((iteration + 1) % fullConfig.humanValidationInterval === 0) {
      console.log(`  ğŸ‘¤ Human Validation...`);
      
      // Select top variations for validation
      const topVariations = batchVariations
        .sort((a, b) => b.aiReward - a.aiReward)
        .slice(0, Math.min(5, batchVariations.length))
        .map(item => ({ variation: item.variation, aiReward: item.aiReward }));
      
      const validationResult = await humanValidate(topVariations, rewardModel);
      
      if (validationResult.validated) {
        improvedPolicy.improvementStats.humanValidations++;
        improvedPolicy.improvementStats.humanCorrections += validationResult.corrections.length;
        
        console.log(`    âœ… Validated: ${validationResult.avgHumanReward.toFixed(4)} avg reward`);
        console.log(`    ğŸ“ Corrections: ${validationResult.corrections.length}`);
      }
    }
    
    // Update policy based on rewards
    improvedPolicy.update(batchRewards);
  }
  
  // Final statistics
  const finalRewards = allRewards.slice(-fullConfig.batchSize);
  improvedPolicy.improvementStats.endingAverageReward = 
    finalRewards.reduce((a, b) => a + b, 0) / finalRewards.length;
  
  console.log('\nâœ… RLAIF Training Complete!');
  console.log(`ğŸ“Š Stats:`, improvedPolicy.improvementStats);
  
  return improvedPolicy;
}

// ============================================================================
// EXPORTS
// ============================================================================

export default {
  rlaifTraining,
  DefaultMutationPolicy,
  bootstrapRewardModel,
  humanValidate,
};
